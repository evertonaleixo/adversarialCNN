{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras import losses\n",
    "# K.tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([2, -1, 3, -2, 2, 2, 1, -4, 5, 1])\n",
    "w = np.array([-1, -1, 1, -1, 1, -1, 1, 1, -1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example showing the phenomenal of fool a binary logistic  regression\n",
    "\n",
    "'p' is the probability of be class 1.\n",
    "If p > 0.5 it belongs to the class 1 otherwise not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04742587317756679\n"
     ]
    }
   ],
   "source": [
    "p = 1/(1+np.power(np.e, -np.dot(x, w)))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try to foll the classifier\n",
    "\n",
    "Doing the dot product again we see that suddenly the score becomes 2. This is not surprising: There are 10 dimensions and we’ve tweaked the input by 0.5 in every dimension in such a way that we gain 0.5 in each one, adding up to a total of 5 additional score, rising it from -3 to 2. Now when we look at probability of class 1 we get 1/(1+e^(-2)) = 0.88. That is, we tweaked the original x by a small amount and we improved the class 1 probability from 5% to 88%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xad = x + 0.5w gives:\n",
    "xad = [1.5, -1.5, 3.5, -2.5, 2.5, 1.5, 1.5, -3.5, 4.5, 1.5]\n",
    "# xad = [1.1, -1.1, 3.1, -2.1, 2.1, 1.1, 1.1, -3.1, 4.1, 1.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1/(1+np.power(np.e, -np.dot(xad, w)))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing the  “Deep MNIST for Experts” tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figures(X_10_samples_2, X_10_noise):\n",
    "    figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "    fig = plt.figure(figsize=(30,15))\n",
    "\n",
    "    idx_img = 0;\n",
    "    for i in range(1, 30, 3):\n",
    "        ax1 = fig.add_subplot(10, 3, i)\n",
    "        ax1.imshow(X_10_samples_2[idx_img])\n",
    "        ax2 = fig.add_subplot(10, 3, i+1)\n",
    "        ax2.imshow(X_10_noise[idx_img])\n",
    "        ax3 = fig.add_subplot(10, 3, i+2)\n",
    "        ax3.imshow( (X_10_samples_2[idx_img] + X_10_noise[idx_img]) )\n",
    "\n",
    "        idx_img = idx_img + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = K.tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 12s 204us/step - loss: 0.2212 - acc: 0.9344\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: 0.0970 - acc: 0.9704\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 12s 200us/step - loss: 0.0686 - acc: 0.9787\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.0542 - acc: 0.9823\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.0431 - acc: 0.9858\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 68us/step\n"
     ]
    }
   ],
   "source": [
    "with K.tf.device('/cpu:0'):\n",
    "    model = K.tf.keras.models.Sequential([\n",
    "      K.tf.keras.layers.Flatten(),\n",
    "      K.tf.keras.layers.Dense(512, activation=K.tf.nn.relu),\n",
    "      K.tf.keras.layers.Dropout(0.2),\n",
    "      K.tf.keras.layers.Dense(10, activation=K.tf.nn.softmax)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "\n",
    "    # for i in range(len(model.layers)):\n",
    "    #     layer = model.layers[i]\n",
    "    #     layer.trainable = False\n",
    "\n",
    "    # model.compile(optimizer='adam',\n",
    "    #           loss='sparse_categorical_crossentropy',\n",
    "    #           metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting samples from the 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5958, 28, 28)\n",
      "(5958,)\n",
      "(10, 28, 28)\n",
      "(10, 28, 28)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "X_samples_2 = x_train[y_train==2]\n",
    "X_10_samples_2 = X_samples_2[:10]\n",
    "X_10_noise = np.zeros((10, 28, 28))\n",
    "X_10_noise = X_10_noise\n",
    "\n",
    "Y_samples_2 = y_train[y_train==2]\n",
    "Y_fakes = np.copy(Y_samples_2[:10])\n",
    "Y_fakes[:] = 6\n",
    "\n",
    "print(X_samples_2.shape)\n",
    "print(Y_samples_2.shape)\n",
    "print(X_10_samples_2.shape)\n",
    "print(X_10_noise.shape)\n",
    "print(Y_fakes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the fisrt way to try fool the Neural Network I will follow this steps.\n",
    "* Get image samples of 2 and target it as 6\n",
    "* Apply the feedforward in CNN\n",
    "* Get the gradient and apply it on image\n",
    "* Get the difference of new image and original image as noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_3/Softmax:0\", shape=(?, 10), dtype=float32, device=/device:CPU:0)\n",
      "(?, 28, 28)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "# Getting gradient\n",
    "input_img = model.input\n",
    "fake_class = K.zeros(shape=(1, 10))\n",
    "\n",
    "layer = model.layers[-1]\n",
    "print(layer.output)\n",
    "\n",
    "# adversarial loss\n",
    "adversarial_loss = losses.categorical_crossentropy(fake_class, layer.output)\n",
    "\n",
    "grads = K.gradients(adversarial_loss, model.input)[0] \n",
    "grads /= K.maximum(K.mean(K.abs(grads)), K.epsilon())\n",
    "\n",
    "print(input_img.shape)\n",
    "print(fake_class.shape)\n",
    "grads_func = K.function(inputs=[input_img, fake_class], outputs=[grads])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  - predicted -  2\n",
      "1  - predicted -  2\n",
      "2  - predicted -  2\n",
      "3  - predicted -  2\n",
      "4  - predicted -  2\n",
      "5  - predicted -  2\n",
      "6  - predicted -  2\n",
      "7  - predicted -  2\n",
      "8  - predicted -  2\n",
      "9  - predicted -  2\n"
     ]
    }
   ],
   "source": [
    "for sample in range(10):\n",
    "    predicted = np.argmax(model.predict(np.array([(X_10_samples_2[sample] + X_10_noise[sample])])))\n",
    "    print(sample, ' - predicted - ', predicted)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Error while reading resource variable dense_3/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/dense_3/bias/N10tensorflow3VarE does not exist.\n\t [[Node: dense_3/BiasAdd/ReadVariableOp = ReadVariableOp[dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_3/bias)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-95ca1ca82a52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_fakes_hot_ones\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'  -- gradient: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Error while reading resource variable dense_3/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/dense_3/bias/N10tensorflow3VarE does not exist.\n\t [[Node: dense_3/BiasAdd/ReadVariableOp = ReadVariableOp[dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_3/bias)]]"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    for sample in range(10):\n",
    "        Y_fakes_hot_ones = np.zeros((1,10))\n",
    "        Y_fakes_hot_ones[0, 6] = 1\n",
    "        \n",
    "        sample_ = np.reshape((X_10_samples_2[sample] + X_10_noise[sample]), (1, 28, 28))\n",
    "        \n",
    "        print(sample_.shape)\n",
    "        grads = grads_func([sample_, Y_fakes_hot_ones])\n",
    "        print('sample ', sample, '  -- gradient: ', grads[0].shape)\n",
    "        \n",
    "        #Update noise\n",
    "        print(X_10_noise.dtype)\n",
    "        fuzzy_img = (X_10_samples_2[sample] + X_10_noise[sample])\n",
    "        print(fuzzy_img.shape)\n",
    "        print(X_10_noise[sample].shape)\n",
    "        X_10_noise[sample] = X_10_noise[sample] - 0 * (grads * fuzzy_img)\n",
    "        \n",
    "        predicted = np.argmax(model.predict(np.array([(X_10_samples_2[sample] + X_10_noise[sample])])))\n",
    "        print('predicted - ', predicted)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fisrt_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-d781e18e9ee2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mW_l1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfisrt_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mW_l1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_l1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# print(W_l1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fisrt_layer' is not defined"
     ]
    }
   ],
   "source": [
    "pred = model.predict((X_10_samples_2 + X_10_noise))\n",
    "pred = np.array([np.argmax(pred[i]) for i in range(10)])\n",
    "count = 0\n",
    "\n",
    "W_l1 = fisrt_layer.get_weights()[0][:,:10].T\n",
    "W_l1 = np.reshape(W_l1, (10, 28, 28))\n",
    "# print(W_l1)\n",
    "\n",
    "# print(X_10_noise[0])\n",
    "# print(X_10_samples_2[0])\n",
    "loss = 100\n",
    "while np.sum(pred == 6) != 10 and count < 100:\n",
    "    print(count , '  - ', np.sum(pred==6), ' - ', pred, loss)\n",
    "    count = count + 1\n",
    "    # Calculate a loss\n",
    "    history = model.fit(X_samples_2, Y_fakes, epochs=1, verbose=0)\n",
    "    loss = history.history['loss'][0]\n",
    "    # Apply this loss on noise\n",
    "    X_10_noise = X_10_noise - ((W_l1 * loss))\n",
    "    \n",
    "    pred = model.predict((X_10_samples_2 + X_10_noise))\n",
    "    pred = np.array([np.argmax(pred[i]) for i in range(10)])\n",
    "    \n",
    "print(X_10_noise[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures(X_10_samples_2, X_10_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying AG approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_population(n):\n",
    "    pop = []\n",
    "    for i in range(n):\n",
    "        individual = np.array([np.random.normal()/5 for _ in range(28*28)])\n",
    "        pop.append(individual)\n",
    "        \n",
    "    return np.array(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict((X_10_noise))\n",
    "for i in range(10):\n",
    "    print( np.argmax( pred[i]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "180.15+(5*86.56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_population(pop):\n",
    "    ranking = []\n",
    "    for individual in pop:\n",
    "        X_10_noise = np.reshape(individual, (28, 28))\n",
    "        new_element = (X_10_samples_2 + X_10_noise)\n",
    "        individual = np.maximum(individual, 0)\n",
    "        individual = np.minimum(individual, 1)\n",
    "\n",
    "        pred = model.predict(new_element)\n",
    "        value = np.sum(pred, axis=0)[6]\n",
    "        ranking.append(value)\n",
    "        \n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutation(individual, probability):\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < probability:\n",
    "            individual[i] = individual[i] + np.random.normal()/10\n",
    "\n",
    "    individual = np.maximum(individual, 0)\n",
    "    individual = np.minimum(individual, 1)\n",
    "    return np.array(individual)\n",
    "\n",
    "def crossover(ind1 , ind2):\n",
    "    position = np.random.randint(len(ind1))\n",
    "    new_individual1 = np.concatenate((ind1[:position], ind2[position:]))\n",
    "    new_individual2 = np.concatenate((ind2[:position], ind1[position:]))\n",
    "    return np.array(new_individual1), np.array(new_individual2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epoch = 10000\n",
    "pop_size = 1000\n",
    "\n",
    "pop = create_population(pop_size)\n",
    "best_val = []\n",
    "best_number_of_six = []\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    ranking = np.array(evaluate_population(pop))\n",
    "    ordered_idx = ranking.argsort()\n",
    "    pop_ordered = pop[ordered_idx]\n",
    "    top5 = np.array(pop_ordered[-5:])\n",
    "    other_ind = np.array(pop[ordered_idx][:-5])\n",
    "    best_val.append(ranking[ordered_idx][-1])\n",
    "    \n",
    "    new_pop = []\n",
    "    for _ in range(len(ordered_idx)):\n",
    "        ind1 = other_ind[np.random.randint(len(other_ind))]\n",
    "        ind2 = other_ind[np.random.randint(len(other_ind))]\n",
    "        new_ind1, new_ind2 = crossover(ind1, ind2)\n",
    "        new_pop.append(new_ind1)\n",
    "        new_pop.append(new_ind2)\n",
    "    \n",
    "    pop_with_mutation = []\n",
    "    for i in range(len(pop)-5):\n",
    "        pop_with_mutation.append(mutation(new_pop[i], probability=0.05))\n",
    "\n",
    "    pop = np.concatenate((top5, pop_with_mutation))\n",
    "    if (epoch % 50)==0:\n",
    "        for top in top5:\n",
    "            top_noise = np.reshape(top, (28, 28))\n",
    "            new_element = (X_10_samples_2 + top_noise)\n",
    "            new_element = np.maximum(new_element, 0)\n",
    "            new_element = np.minimum(new_element, 1)\n",
    "\n",
    "            pred = model.predict(new_element)\n",
    "            pred = np.array([np.argmax(pred[i]) for i in range(10)])\n",
    "            value = np.sum(pred==6)\n",
    "            best_number_of_six.append(value)\n",
    "        print(epoch, ' - best value: ', best_val[len(best_val) - 1])\n",
    "        print(best_number_of_six)\n",
    "        print()\n",
    "        \n",
    "    if np.max(np.array(best_number_of_six)) == 10:\n",
    "        break\n",
    "    \n",
    "\n",
    "ranking = np.array(evaluate_population(pop))\n",
    "ordered_idx = ranking.argsort()\n",
    "top5 = np.array(pop[ordered_idx][-5:])\n",
    "\n",
    "print(top5)\n",
    "print(ranking)\n",
    "print(ranking[ordered_idx][-5])\n",
    "print(best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_10_noise = np.zeros((10, 28, 28))\n",
    "for i in range(10):\n",
    "    X_10_noise[i] = np.reshape(top5[0], (28,28))\n",
    "\n",
    "new_element = (X_10_samples_2 + np.reshape(top5[0], (28,28)))\n",
    "pred = model.predict(new_element)\n",
    "pred = np.array([np.argmax(pred[i]) for i in range(10)])\n",
    "\n",
    "\n",
    "print(pred)\n",
    "plt.hist(X_10_samples_2[0], label='X_10_samples')\n",
    "# plt.hist(X_10_noise[0], label='Noise')\n",
    "plot_figures(X_10_samples_2, X_10_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying an Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_actions = (28*28*100)\n",
    "\n",
    "def get_state(noise):\n",
    "    noise = np.copy(noise)\n",
    "    noise[noise>.9] = 1\n",
    "    noise[noise<=.9 and noise>.8] = .9\n",
    "    noise[noise<=.8 and noise>.7] = .8\n",
    "    noise[noise<=.7 and noise>.6] = .7\n",
    "    noise[noise<=.6 and noise>.5] = .6\n",
    "    noise[noise<=.5 and noise>.4] = .5\n",
    "    noise[noise<=.4 and noise>.3] = .4\n",
    "    noise[noise<=.3 and noise>.2] = .3\n",
    "    noise[noise<=.2 and noise>.1] = .2\n",
    "    noise[noise<=.1 and noise>.0] = .1\n",
    "    noise[noise<0] = 0\n",
    "    \n",
    "    return noise\n",
    "\n",
    "def Q_function(state, noise, action):\n",
    "    X_10_noise = np.reshape(individual, (28, 28))\n",
    "    pred = model.predict((X_10_samples_2 + X_10_noise))\n",
    "    value = np.sum(pred, axis=0)[6]\n",
    "    \n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_action(state):\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_times = 10\n",
    "\n",
    "for num_train_time in range(num_train_times):\n",
    "    initial_noise = np.array([np.random.normal()/5 for _ in range(28*28)])\n",
    "    initial_state = get_state(initial_noise)\n",
    "    state = np.array(initial_state)\n",
    "    noise = np.array(initial_noise)\n",
    "    \n",
    "    for _ in range(max_actions):\n",
    "        # Select an action (Explore or Exploit)\n",
    "        action = -1\n",
    "        if np.random.rand() < .2: # Explore\n",
    "            action = np.random.randint((28*28))\n",
    "        else:\n",
    "            action = get_best_action(state)\n",
    "        \n",
    "        # Apply Action\n",
    "        noise[action] = noise[action] + np.random.normal()/10\n",
    "        # Getting Reward\n",
    "        top_noise = np.reshape(noise, (28, 28))\n",
    "        new_element = (X_10_samples_2 + top_noise)\n",
    "        for pos in range(X_10_samples_2.shape[0]):\n",
    "            for x in range(X_10_samples_2.shape[1]):\n",
    "                for y in range(X_10_samples_2.shape[2]):\n",
    "                    new_element[pos][x][y] = 0 if new_element[pos][x][y] < 0 else new_element[pos][x][y]\n",
    "                    new_element[pos][x][y] = 1 if new_element[pos][x][y] > 1 else new_element[pos][x][y]\n",
    "\n",
    "        pred = model.predict(new_element)\n",
    "        pred = np.array([np.argmax(pred[i]) for i in range(10)])\n",
    "        reward = np.sum(pred==6)\n",
    "        if reward > 9.5:\n",
    "            break\n",
    "        \n",
    "        # Update Q_Function\n",
    "        # Generating new state\n",
    "        state = get_state(noise)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
